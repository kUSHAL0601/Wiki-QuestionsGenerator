{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, nltk, sys, wikipedia\n",
    "\n",
    "def getNodes(parent):\n",
    "    allLeaves = []\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            allLeaves.append(node.leaves())\n",
    "            getNodes(node)\n",
    "    allLeaves.sort(key=len, reverse=True)\n",
    "    return allLeaves\n",
    "\n",
    "def data(topic):\n",
    "    page=wikipedia.page(topic)\n",
    "    return page.content\n",
    "\n",
    "x=data(\"India\").split('\\n')\n",
    "blocks=[]\n",
    "for i in x:\n",
    "    if i and not \"=\" in i:\n",
    "        blocks.append(i)\n",
    "        \n",
    "sentences=[]\n",
    "for i in blocks:\n",
    "    sentences+=i.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['species'], ['evolve']]\n",
      "Darwin studied how species evolve \\s(species)[\\s|,|:|-]\n",
      " Darwin studied how species evolve  \\s(species)[\\s|,|:|-]\n",
      "Darwin studied how *#$% evolve \\s(species)[\\s|,|:|-]\n",
      "Darwin studied how *#$% evolve \\s(evolve)[\\s|,|:|-]\n",
      " Darwin studied how *#$% evolve  \\s(evolve)[\\s|,|:|-]\n",
      "Darwin studied how *#$% *#$% \\s(evolve)[\\s|,|:|-]\n",
      "[('Darwin', 'NNP', 'PERSON'), ('studied', 'VBD', 'O'), ('how', 'WRB', 'O')]\n",
      "[]\n",
      "[]\n",
      "[[('who', 'Darwin')], [], []]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "\n",
    "def outputQPhrase(phraseList):\n",
    "    nounPhrases = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "    posTagList = [phrase[1] for phrase in phraseList]\n",
    "    output = []\n",
    "    for (i,wordTuple) in enumerate(phraseList):\n",
    "        if wordTuple[1] == \"PRP\" or (wordTuple[2] == \"PERSON\" and wordTuple[1] in nounPhrases):\n",
    "            output.append((\"who\", wordTuple[0]))\n",
    "        if not (wordTuple[2] == \"PERSON\" or wordTuple[2] == \"TIME\") and wordTuple[1] in nounPhrases:\n",
    "            output.append((\"what\", wordTuple[0]))\n",
    "        if wordTuple[2] == \"GPE\" and \"IN\" in posTagList and wordTuple[1] in nounPhrases:\n",
    "            inIndex = posTagList.index(\"IN\")\n",
    "            if inIndex < i and phraseList[inIndex] in [\"on\", \"in\", \"at\", \"over\", \"to\"]:\n",
    "                output.append((\"where\", wordTuple[0]))\n",
    "        if (wordTuple[2] == \"TIME\" and wordTuple[1] in nounPhrases) or (re.match(\"[1|2]\\d\\d\\d$\",wordTuple[0])):\n",
    "            output.append((\"when\", wordTuple[0]))\n",
    "        if (wordTuple[2] == \"PERSON\" and wordTuple[1] in nounPhrases) and i<(len(phraseList)-1) and phraseList[i+1][1] == \"POS\":\n",
    "            try:\n",
    "                nextNPIdx = [posTagList.index(i) for i in posTagList[i+1:] if i in nounPhrases][0]\n",
    "                output.append((\"whose \" + phraseList[nextNPIdx][0], wordTuple[0]))\n",
    "            except:\n",
    "                pass\n",
    "        if (wordTuple[1] == \"CD\"):\n",
    "            try:\n",
    "                nextNPIdx = [posTagList.index(i) for i in posTagList[i+1:] if i in nounPhrases][0]\n",
    "                output.append((\"how many \" + phraseList[nextNPIdx][0], wordTuple[0]))\n",
    "            except:\n",
    "                pass\n",
    "    return output\n",
    "\n",
    "def getNodes(parent):\n",
    "    allLeaves = []\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            allLeaves.append(node.leaves())\n",
    "            getNodes(node)\n",
    "    allLeaves.sort(key=len, reverse=True)\n",
    "    return allLeaves\n",
    "\n",
    "def nerTagging(sentence):\n",
    "    sent = tree2conlltags(ne_chunk(pos_tag(word_tokenize(sentence))))\n",
    "    temp = []\n",
    "    skip_next = False\n",
    "    for (i,tag) in enumerate(sent):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if \"-\" in tag[2]:\n",
    "            if (i+1) <len(sent) and '-' in sent[i+1][2] and tag[2].split('-')[1]==sent[i+1][2].split('-')[1]:\n",
    "                skip_next = True\n",
    "                temp.append((tag[0] + \" \" + sent[i+1][0], tag[1], tag[2].split(\"-\")[1]))\n",
    "            else:\n",
    "                temp.append((tag[0], tag[1], tag[2].split(\"-\")[1]))\n",
    "        else:\n",
    "            temp.append((tag[0], tag[1], tag[2]))\n",
    "    output = []\n",
    "    print (temp)\n",
    "    return outputQPhrase(temp)\n",
    "\n",
    "rules = [\n",
    "    \"VP < (S=unmv $,, /,/)\",\n",
    "    \"S < PP|ADJP|ADVP|S|SBAR=unmv > ROOT\",\n",
    "    \"/\\\\.*/ < CC << NP|ADJP|VP|ADVP|PP=unmv\",\n",
    "    \"SBAR < (IN|DT < /[^that]/) << NP|PP=unmv\",\n",
    "    \"SBAR < /^WH.*P$/ << NP|ADJP|VP|ADVP|PP=unmv\",\n",
    "    \"SBAR <, IN|DT < (S < (NP=unmv !$,, VP))\",\n",
    "    \"NP << (PP=unmv !< (IN < of|about))\",\n",
    "    \"PP << PP=unmv\",\n",
    "    \"NP $ VP << PP=unmv\",\n",
    "    \"SBAR=unmv [ !> VP | $-- /,/ | < RB ]\",\n",
    "    \"SBAR=unmv !< WHNP <(/^[^S].*/ !<< that|whether|how)\",\n",
    "    \"NP=unmv < EX\",\n",
    "    \" /^S/ < '' << NP|ADJP|VP|ADVP|PP=unmv\",\n",
    "    \"PP=unmv !< NP\",\n",
    "    \"NP=unmv $ @NP\",\n",
    "    \"NP|PP|ADJP|ADVP << NP|ADJP|VP|ADVP|PP=unmv\",\n",
    "    \"@UNMV << NP|ADJP|VP|ADVP|PP=unmv\"\n",
    "]\n",
    "url = \"http://localhost:9010/tregex\"\n",
    "treesList = []\n",
    "# data = [\"The companies' profits doubled in the past year.\"]\n",
    "data = [\"Darwin studied how species evolve\"]\n",
    "# text = sentences[0]\n",
    "text = data[0]\n",
    "temp = []\n",
    "for rule in rules:\n",
    "    request_params = {\"pattern\": rule}\n",
    "    r = requests.post(url, data=text.encode('utf-8'), params=request_params)\n",
    "    s = r.json()\n",
    "    temp.append(list(s['sentences']))\n",
    "\n",
    "unmovableWords = []\n",
    "for j in temp:\n",
    "    for k in j:\n",
    "        for l in k:\n",
    "            if 'namedNodes' in k[l]:\n",
    "                if 'unmv' in k[l]['namedNodes'][0]:\n",
    "                    string = k[l]['namedNodes'][0]['unmv']\n",
    "                    unmovableWords.extend(getNodes(Tree.fromstring(string)))\n",
    "unmovableWords.sort(key=len, reverse=True)\n",
    "# print (unmovableWords)\n",
    "finalList = []\n",
    "exceptions = [\"-LRB-\", \"-RRB-\", \":\", \",\"]\n",
    "for phrase in unmovableWords:\n",
    "    tmp = []\n",
    "    for word in phrase:\n",
    "        if word not in exceptions:\n",
    "            tmp.append(word)\n",
    "    finalList.append(tmp)\n",
    "finalList.sort(key=len, reverse=True)\n",
    "print (finalList)\n",
    "sentExceptions = [\"(\",\")\",\":\",\",\"]\n",
    "for sentExcept in sentExceptions:\n",
    "    text = text.replace(sentExcept, \"\")\n",
    "\n",
    "for phraseList in finalList:\n",
    "    phrase = \" \".join(phraseList)\n",
    "    pattern = \"\\s(\" + phrase + \")[\\s|,|:|-]\"\n",
    "    print (text, pattern)\n",
    "    tp = \" \" + text + \" \"\n",
    "    print (tp, pattern)\n",
    "    text = re.sub(pattern, \" *#$% \", tp).strip()\n",
    "    print (text, pattern)\n",
    "\n",
    "allPhrases = text.split(\"*#$%\")\n",
    "questionsList = []\n",
    "for phrase in allPhrases:\n",
    "    questionsList.append(nerTagging(phrase))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India *#$% BhÄrat also known *#$% is a country in South Asia'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (i,text) in enumerate(data):\n",
    "    temp = []\n",
    "    for rule in rules:\n",
    "        request_params = {\"pattern\": rule}\n",
    "        r = requests.post(url, data=text.encode('utf-8'), params=request_params)\n",
    "        s = r.json()\n",
    "        # print (s['sentences'])\n",
    "        temp.append(list(s['sentences']))\n",
    "    unmovableWords = []\n",
    "    for j in temp:\n",
    "        for k in j:\n",
    "            for l in k:\n",
    "                if 'namedNodes' in k[l]:\n",
    "                    if 'unmv' in k[l]['namedNodes'][0]:\n",
    "                        string = k[l]['namedNodes'][0]['unmv']\n",
    "                        unmovableWords.extend(getNodes(Tree.fromstring(string)))\n",
    "    unmovableWords.sort(key=len, reverse=True)\n",
    "    finalList = []\n",
    "    exceptions = [\"-LRB-\", \"-RRB-\", \":\", \",\"]\n",
    "    for phrase in unmovableWords:\n",
    "        tmp = []\n",
    "        for word in phrase:\n",
    "            if word not in exceptions:\n",
    "                tmp.append(word)\n",
    "        finalList.append(tmp)\n",
    "    finalList.sort(key=len, reverse=True)\n",
    "    sentExceptions = [\"(\",\")\",\":\",\",\"]\n",
    "    for sentExcept in sentExceptions:\n",
    "        text = text.replace(sentExcept, \"\")\n",
    "\n",
    "    for phraseList in finalList:\n",
    "        phrase = \" \".join(phraseList)\n",
    "        pattern = \"\\s\" + phrase + \"\\s\"\n",
    "        text = re.sub(pattern, \"*#$%\", text)\n",
    "\n",
    "    allPhrases = text.split(\"*#$%\")\n",
    "    print (allPhrases)\n",
    "    for phrase in allPhrases:\n",
    "        nerTagging(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
